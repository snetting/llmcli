#!/usr/bin/env python3
import argparse
import sys
import os
import requests
import re
import subprocess
import json
import shutil
import time

# === CONFIGURATION VARIABLES ===
# Safer to define via ENVARS, but can also include defaults here
#API = os.getenv("LLM_API", "https://openwebui.url/api/chat/completions")
#API_TOKEN = os.getenv("LLM_API_TOKEN", "sk-xxx")
#LLM_MODEL = os.getenv("LLM_MODEL", "qwen-general14b")
#COLLECTION_ID = os.getenv("LLM_COLLECTION_ID", "")  # Leave empty if unused
# ===============================


SYSTEM_PROMPT = (
    "You are an expert in Linux administration and automation. "
    "You can execute shell commands to achieve tasks, and you will receive the output of each command. Never run interactive commands — if you need CPU or memory stats, use non-interactive tools (e.g. top -b -n1, vmstat, reading /proc/stat, or using a Python psutil snippet). "
    "Respond only in JSON when running in shell-task mode."
)


# Global verbosity-controlled flags (set at runtime)
SHOW_STATUS = False   # -v
SHOW_THINKING = False # -vv  
SHOW_COMMANDS = False # -vvv
INTERACTIVE = False   # -i

# Internal: track last status length for clearing
default_status_len = 0
_last_status_len = 0

def print_status(message: str):
    """Print each status update on its own line."""
    print(f"[STATUS] {message}")


def read_stdin_if_available():
    if not sys.stdin.isatty():
        return sys.stdin.read()
    return None


def read_file_contents(path):
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()


def strip_thinking_blocks(text):
    return re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()


def call_chat_api(messages):
    headers = {"Authorization": f"Bearer {API_TOKEN}", "Content-Type": "application/json"}
    payload = {"model": LLM_MODEL, "messages": messages}
    if COLLECTION_ID:
        payload["collection_id"] = COLLECTION_ID

    if SHOW_STATUS:
        print_status("Thinking…")
    resp = requests.post(API, headers=headers, json=payload)
    resp.raise_for_status()
    data = resp.json()

    raw = data["choices"][0]["message"]["content"]
    if SHOW_THINKING:
        print(raw.strip())

    return raw, data


def run_shell_command(cmd):
    """Execute a shell command with timeout, return (stdout, stderr, returncode)."""
    try:
        proc = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
        return proc.stdout, proc.stderr, proc.returncode
    except subprocess.TimeoutExpired:
        return "", f"Command timed out after 30s", -1


def run_shell_agent(task):
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": (
            f"Your task is: {task}\n\n"
            "For each step, output {\"command\": <shell command>, \"explanation\": <why>}.\n"
            "When complete, output {\"done\": true} and stop. Output only JSON."
        )}
    ]
    decoder = json.JSONDecoder()
    collected = []
    done = False

    for _ in range(50):
        raw, _ = call_chat_api(messages)
        clean = strip_thinking_blocks(raw)

        # decode JSON objects
        actions, idx = [], 0
        while idx < len(clean):
            try:
                obj, idx = decoder.raw_decode(clean, idx)
            except json.JSONDecodeError:
                break
            if isinstance(obj, list):
                actions.extend(obj)
            else:
                actions.append(obj)
            while idx < len(clean) and clean[idx].isspace():
                idx += 1

        if not actions:
            print(f"❌ Failed to parse JSON from LLM:\n{clean}")
            return

        for act in actions:
            if act.get("done"):
                done = True
                break
            cmd = act.get("command")
            why = act.get("explanation", "")
            if not cmd:
                continue

            # status line for command
            if SHOW_STATUS:
                print_status(f"Running command: {cmd}")

            # interactive confirmation
            if INTERACTIVE:
                print(f"About to run:\n  {cmd}\nReason: {why}")
                resp = input("Proceed? [y/N] ").strip().lower()
                if resp not in ("y", "yes"):
                    print("⏹  Aborted by user.")
                    sys.exit(1)

            if SHOW_COMMANDS:
                print(f">>> COMMAND: {cmd}\n# {why}")

            out, err, code = run_shell_command(cmd)
            result = (
                f"COMMAND: {cmd}\n"
                f"STDOUT:\n{out}\n"
                f"STDERR:\n{err}\n"
                f"RETURN CODE: {code}"
            )
            if SHOW_COMMANDS:
                print(result)

            collected.append(result)
            messages.append({
                "role": "user",
                "content": f"<command_output>\n{result}\n</command_output>"
            })

        if done:
            break

    if not done:
        print("❌ Reached max iterations (50). Exiting.")
        return

    # final summary
    if SHOW_STATUS:
        print_status("Creating summary…")
    #messages.append({"role": "user", "content":
    #    "Based on the command outputs above, provide a concise summary of the operations performed and final result."
    #})
    # force JSON-only on the summary turn:
    messages.append({
        "role": "system",
        "content": (
            "Now output a single JSON object with exactly two fields:\n"
            "  \"done\": true,\n"
            "  \"summary\": a concise plain-text summary\n"
            "Do not output anything else."
        )
    })
    messages.append({
        "role": "user",
        "content":
            "Based on the command outputs above, provide a concise summary of the operations performed and final result."
    })
    final_raw, _ = call_chat_api(messages)
    summary = final_raw.strip() if SHOW_THINKING else strip_thinking_blocks(final_raw)
    print(f"\n✅ Final summary: {summary}")

    # Originally reported raw output if JSON fails, but this almost always causes additional output :(
    #final_raw, _ = call_chat_api(messages)
    # (2) safely parse JSON, fallback to raw text
    #try:
    #    final_obj = json.loads(final_raw)
    #    summary = final_obj.get("summary", "").strip()
    #    if not summary:
    #        raise ValueError("empty summary field")
    #except Exception as e:
    #    # model either returned non-JSON or an empty JSON; fallback:
    #    print("[WARN] failed to parse JSON summary:", e)
    #    print("[DEBUG] raw LLM reply:", final_raw)
    #    summary = strip_thinking_blocks(final_raw)
    #    print(f"\n✅ Final summary: {summary}")


def main():
    parser = argparse.ArgumentParser(
        description="CLI for querying an LLM chat endpoint or running shell-agent tasks."
    )
    parser.add_argument("-v", "--verbose",
                        action="count", default=0,
                        help="Increase verbosity: -v status line, -vv show thinking, -vvv show commands+output")
    parser.add_argument("-i", "--interactive",
                        action="store_true",
                        help="Ask for confirmation before running each shell command")
    parser.add_argument("-s", "--shell-task", dest="shell_task",
                        help="Automate a shell task end-to-end.")
    parser.add_argument("-f", "--file", help="Include file content as context.")
    parser.add_argument("question", nargs="?", help="Question or prompt for the model.")

    args = parser.parse_args()

    # map args to globals
    global SHOW_STATUS, SHOW_THINKING, SHOW_COMMANDS, INTERACTIVE
    SHOW_STATUS   = args.verbose >= 1
    SHOW_THINKING = args.verbose >= 2
    SHOW_COMMANDS = args.verbose >= 3
    INTERACTIVE   = args.interactive

    if args.shell_task:
        run_shell_agent(args.shell_task)
        return

    if not args.question:
        parser.print_help(sys.stderr)
        sys.exit(1)

    # prepare context
    q = args.question
    if args.file:
        if not os.path.isfile(args.file):
            print(f"Error: file not found: {args.file}", file=sys.stderr)
            sys.exit(1)
        content = read_file_contents(args.file)
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": f"Here is the file content:\n{content}\nUser question: {q}"}
        ]
    else:
        piped = read_stdin_if_available()
        messages = [{"role": "system", "content": SYSTEM_PROMPT}]
        if piped:
            messages.append({"role": "user", "content": piped})
        messages.append({"role": "user", "content": q})

    if SHOW_STATUS:
        print_status("Thinking…")
    raw, _ = call_chat_api(messages)

    output = raw.strip() if SHOW_THINKING else strip_thinking_blocks(raw)
    print(output)


if __name__ == "__main__":
    main()

